{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver as webdriver\n",
    "\n",
    "#path for driver\n",
    "DRIVER_PATH = \"/Users/cshingay/Desktop/1_CODING/chromedriver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% code to scroll to bottom on a site\n"
    }
   },
   "outputs": [],
   "source": [
    "def scroll(driver, timeout):\n",
    "    scroll_pause_time = timeout\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(scroll_pause_time)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_links(url, keyword):\n",
    "    wd = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "    wd.implicitly_wait(30)\n",
    "    wd.get(url)\n",
    "\n",
    "    # Uncomment this line if you want to scroll the page before scraping\n",
    "    # scroll(wd, 2)\n",
    "\n",
    "    soup_a = bs(wd.page_source, \"lxml\")\n",
    "    wd.close()\n",
    "\n",
    "    links = []\n",
    "    for link in soup_a.find_all('a'):\n",
    "        try:\n",
    "            link_text = link.get('href')\n",
    "            if link_text and keyword in link_text:  # Check if the keyword is in the link\n",
    "                links.append(link_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    \n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% GETTING LINKS FROM PAGES\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBASE_LINK = \"https://directory.goodonyou.eco\"\\ngiven_store = \"/brand/mayamiko\"\\n\\n#def get_shop_site(store):\\nwd = webdriver.Chrome(executable_path = DRIVER_PATH)\\nwd.get(BASE_LINK + given_store)\\nwd.find_element_by_link_text(\"Shop Online\").click()\\n\\nsoup =  bs(wd.page_source, \\'lxml\\')\\nlink = soup.find_all(\\'a\\', class_ = \"sc-bxivhb kGVRLw\")[4].get(\"href\")\\nprint(link)\\n\\n#wd.close()\\n\\n#print(get_shop_site(given_store))\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = [\n",
    "    \"https://chnge.com/?utm_source=GoodOnYou%20App&utm_medium=Referral\",\n",
    "    \"https://eclipseglove.com/?utm_campaign=GoY-rating&utm_medium=referral&utm_source=GoodOnYou&utm_content=brandpage\",\n",
    "    \"https://www.honest-basics.com/?utm_campaign=GoY-rating&utm_medium=referral&utm_source=GoodOnYou&utm_content=brandpage\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1516\n",
      "2877\n",
      "4259\n",
      "5621\n",
      "6992\n",
      "8354\n",
      "9715\n",
      "11077\n",
      "12449\n",
      "13811\n",
      "15183\n",
      "16554\n",
      "17936\n",
      "19308\n",
      "20670\n",
      "22041\n",
      "23403\n",
      "24775\n",
      "26147\n",
      "27528\n",
      "28890\n",
      "30252\n",
      "31623\n",
      "32985\n",
      "34356\n",
      "https://chnge.com/?utm_source=GoodOnYou%20App&utm_medium=Referral\n",
      "3368\n",
      "https://eclipseglove.com/?utm_campaign=GoY-rating&utm_medium=referral&utm_source=GoodOnYou&utm_content=brandpage\n",
      "7973\n",
      "8024\n",
      "https://www.honest-basics.com/?utm_campaign=GoY-rating&utm_medium=referral&utm_source=GoodOnYou&utm_content=brandpage\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "\n",
    "scroll_items = []\n",
    "\n",
    "# Use updated method to initialize Chrome driver\n",
    "wd = webdriver.Chrome(service=Service(DRIVER_PATH))\n",
    "\n",
    "for link in links:\n",
    "    try:\n",
    "        # Check if the link contains a valid slash after domain\n",
    "        ind = link.index(\"/\", link.index(\".\"))\n",
    "        use_link = link[0:ind] + \"/collections/all/\"\n",
    "        \n",
    "        wd.get(use_link)\n",
    "        scroll(wd, 2)\n",
    "        soup = bs(wd.page_source, \"lxml\")\n",
    "        \n",
    "        print(link)\n",
    "\n",
    "        for item_link in soup.find_all('a'):\n",
    "            href = item_link.get(\"href\")\n",
    "            if href and \"products\" in str(item_link):\n",
    "                scroll_items.append(link[0:ind] + href)\n",
    "    except ValueError:\n",
    "        print(f\"Invalid URL format: {link}\")\n",
    "        continue\n",
    "\n",
    "wd.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Ordering in categories:\n"
    }
   },
   "outputs": [],
   "source": [
    "shirt = [\"shirt\", \"tee\", \"tank\", \"henley\", \"blouse\"]\n",
    "dress = [\"dress\", \"jumpsuit\", \"midi\", \"mini\", \"maxi\"]\n",
    "bottom = [ \"jean\", \"trouser\", \"culotte\", \"shorts\"]\n",
    "\n",
    "products = pd.DataFrame(scroll_items).drop_duplicates()\n",
    "products = products.rename(columns = {0:\"links\"})\n",
    "\n",
    "shirts = products[products[\"links\"].str.contains('|'.join(shirt), case=False, na=False)]\n",
    "dresses = products[products[\"links\"].str.contains('|'.join(dress), case=False, na=False)]\n",
    "bottoms = products[products[\"links\"].str.contains('|'.join(bottom), case=False, na=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Getting & inserting photo links into csv files\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# Define Driver Path\n",
    "DRIVER_PATH = \"D:\\\\chromedriver\\\\chromedriver.exe\"  # Update with your actual path if needed\n",
    "\n",
    "# Load CSVs with links\n",
    "bottoms = pd.read_csv(r\"D:\\mini_project\\HACKATHON\\GreenStyle-main\\bottoms_22.csv\")\n",
    "shirts = pd.read_csv(r\"D:\\mini_project\\HACKATHON\\GreenStyle-main\\tops_22.csv\")\n",
    "dresses = pd.read_csv(r\"D:\\mini_project\\HACKATHON\\GreenStyle-main\\dresses_22.csv\")\n",
    "\n",
    "# Initialize Chrome WebDriver\n",
    "photo_link = []\n",
    "wd = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "\n",
    "# ------------------------ Process Bottoms ------------------------\n",
    "for link in bottoms['links']:\n",
    "    wd.get(link)\n",
    "    soup = bs(wd.page_source, \"lxml\")\n",
    "\n",
    "    for item_link in soup.find_all('img'):\n",
    "        item = item_link.get('src')\n",
    "        if item and \"products\" in item and \"shopify\" in item:\n",
    "            photo_link.append(item)\n",
    "            break\n",
    "\n",
    "bottoms.insert(1, \"Photos\", photo_link)\n",
    "bottoms.to_csv(r\"D:\\mini_project\\HACKATHON\\GreenStyle-main\\bottoms_22.csv\", index=False)\n",
    "\n",
    "# ------------------------ Process Shirts ------------------------\n",
    "photo_link = []  # Reset photo_link for shirts\n",
    "\n",
    "for link in shirts['links']:\n",
    "    wd.get(link)\n",
    "    soup = bs(wd.page_source, \"lxml\")\n",
    "\n",
    "    for item_link in soup.find_all('img'):\n",
    "        item = item_link.get('src')\n",
    "        if item and \"products\" in item and \"shopify\" in item:\n",
    "            photo_link.append(item)\n",
    "            break\n",
    "\n",
    "shirts.insert(1, \"Photos\", photo_link)\n",
    "shirts.to_csv(r\"D:\\mini_project\\HACKATHON\\GreenStyle-main\\tops_22.csv\", index=False)\n",
    "\n",
    "# ------------------------ Process Dresses ------------------------\n",
    "photo_link = []  # Reset photo_link for dresses\n",
    "\n",
    "for link in dresses['links']:\n",
    "    wd.get(link)\n",
    "    soup = bs(wd.page_source, \"lxml\")\n",
    "\n",
    "    for item_link in soup.find_all('img'):\n",
    "        item = item_link.get('src')\n",
    "        if item and \"products\" in item and \"shopify\" in item:\n",
    "            photo_link.append(item)\n",
    "            break\n",
    "\n",
    "dresses.insert(1, \"Photos\", photo_link)\n",
    "dresses.to_csv(r\"D:\\mini_project\\HACKATHON\\GreenStyle-main\\dresses_22.csv\", index=False)\n",
    "\n",
    "# Close the WebDriver\n",
    "wd.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
